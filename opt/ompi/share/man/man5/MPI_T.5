.\" Automatically generated by Pandoc 2.11.3
.\"
.TH "MPI_T" "5" "" "2021-02-04" "Open MPI"
.hy
.SH NAME
.PP
Open MPI\[cq]s MPI_T interface - General information
.SH DESCRIPTION
.PP
There are a few Open MPI-specific notes worth mentioning about its
\f[C]MPI_T\f[R] interface implementation.
.SS MPI_T Control Variables
.PP
Open MPI\[cq]s implementation of the \f[C]MPI_T\f[R] Control Variable
(\[lq]cvar\[rq]) APIs is an interface to Open MPI\[cq]s underlying
Modular Component Architecture (MCA) parameters/variables.
Simply put: using the \f[C]MPI_T\f[R] cvar interface is another
mechanism to get/set Open MPI MCA parameters.
.PP
In order of precedence (highest to lowest), Open MPI provides the
following mechanisms to set MCA parameters:
.IP "1." 3
The \f[C]MPI_T\f[R] interface has the highest precedence.
Specifically: values set via the \f[C]MPI_T\f[R] interface will override
all other settings.
.IP "2." 3
The \f[C]mpirun(1)\f[R] / \f[C]mpiexec(1)\f[R] command line (e.g., via
the \f[C]--mca\f[R] parameter).
.IP "3." 3
Environment variables.
.IP "4." 3
Parameter files have the lowest precedence.
Specifically: values set via parameter files can be overridden by any of
the other MCA-variable setting mechanisms.
.SS MPI initialization
.PP
An application may use the \f[C]MPI_T\f[R] interface before MPI is
initialized to set MCA parameters.
Setting MPI-level MCA parameters before MPI is initialized may affect
\f[I]how\f[R] MPI is initialized (e.g., by influencing which frameworks
and components are selected).
.PP
The following example sets the \f[C]pml\f[R] and \f[C]btl\f[R] MCA
params before invoking \f[C]MPI_Init(3)\f[R] in order to force a
specific selection of PML and BTL components:
.IP
.nf
\f[C]
int provided, index, count;
MPI_T_cvar_handle pml_handle, btl_handle;
char pml_value[64], btl_value[64];

MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);

MPI_T_cvar_get_index(\[dq]pml\[dq], &index);
MPI_T_cvar_handle_alloc(index, NULL, &pml_handle, &count);
MPI_T_cvar_write(pml_handle, \[dq]ob1\[dq]);

MPI_T_cvar_get_index(\[dq]btl\[dq], &index);
MPI_T_cvar_handle_alloc(index, NULL, &btl_handle, &count);
MPI_T_cvar_write(btl_handle, \[dq]tcp,vader,self\[dq]);

MPI_T_cvar_read(pml_handle, pml_value);
MPI_T_cvar_read(btl_handle, btl_value);
printf(\[dq]Set value of cvars: PML: %s, BTL: %s\[rs]n\[dq],
       pml_value, btl_value);

MPI_T_cvar_handle_free(&pml_handle);
MPI_T_cvar_handle_free(&btl_handle);

MPI_Init(NULL, NULL);

// ...

MPI_Finalize();

MPI_T_finalize();
\f[R]
.fi
.PP
Note that once MPI is initialized, most Open MPI cvars become read-only.
.PP
For example, after MPI is initialized, it is no longer possible to set
the PML and BTL selection mechanisms.
This is because many of these MCA parameters are only used during MPI
initialization; setting them after MPI has already been initialized
would be meaningless, anyway.
.SS MPI_T Categories
.PP
Open MPI\[cq]s MPI_T categories are organized hierarchically:
.IP "1." 3
Layer (or \[lq]project\[rq]).
There are two layers in Open MPI:
.RS 4
.IP \[bu] 2
\f[C]ompi\f[R]: This layer contains cvars, pvars, and sub categories
related to MPI characteristics.
.IP \[bu] 2
\f[C]opal\f[R]: This layer generally contains cvars, pvars, and sub
categories of lower-layer constructions, such as operating system
issues, networking issues, etc.
.RE
.IP "2." 3
Framework or section.
.RS 4
.IP \[bu] 2
In most cases, the next level in the hierarchy is the Open MPI MCA
framework.
.RS 2
.IP \[bu] 2
For example, you can find the \f[C]btl\f[R] framework under the
\f[C]opal\f[R] layer (because it has to do with the underlying
networking).
.IP \[bu] 2
Additionally, the \f[C]pml\f[R] framework is under the \f[C]ompi\f[R]
layer (because it has to do with MPI semantics of point-to-point
messaging).
.RE
.IP \[bu] 2
There are a few non-MCA-framework entities under the layer, however.
.RS 2
.IP \[bu] 2
For example, there is an \f[C]mpi\f[R] section under both the
\f[C]opal\f[R] and \f[C]ompi\f[R] layers for general/core MPI
constructs.
.RE
.RE
.IP "3." 3
Component.
.RS 4
.IP \[bu] 2
If relevant, the third level in the hierarchy is the MCA component.
.IP \[bu] 2
For example, the \f[C]tcp\f[R] component can be found under the
\f[C]opal\f[R] framework in the \f[C]opal\f[R] layer.
.RE
.SH SEE ALSO
.PP
\f[C]MPI_T_init\f[R](3), \f[C]MPI_T_finalize\f[R](3)
